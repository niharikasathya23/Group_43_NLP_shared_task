{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "42jyRpJ8LAqt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9679b195-b695-4ecc-9a32-ef8bb374a8c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers==4.9.1\n",
            "  Downloading transformers-4.9.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.6 MB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (2022.6.2)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 38.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (1.21.6)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 60.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (3.8.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers==4.9.1) (6.0)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub==0.0.12->transformers==4.9.1) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->transformers==4.9.1) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.9.1) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.9.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.9.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers==4.9.1) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.9.1) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.9.1) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from sacremoses->transformers==4.9.1) (1.2.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=81385a002cc876c3cccd83fbe8c96c647e47a8edf22f8ca3dc69a33fbdf61227\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.53 tokenizers-0.10.3 transformers-4.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.9.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==1.10.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zonlpmGxbQ4U",
        "outputId": "f0b3be43-05b2-423d-c900-4bbcc826e453"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting datasets==1.10.2\n",
            "  Downloading datasets-1.10.2-py3-none-any.whl (542 kB)\n",
            "\u001b[K     |████████████████████████████████| 542 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets==1.10.2) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets==1.10.2) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets==1.10.2) (21.3)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 54.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.10.2) (2022.11.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.10.2) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from datasets==1.10.2) (0.3.6)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.10.2) (0.0.12)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets==1.10.2) (9.0.0)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 69.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.8/dist-packages (from datasets==1.10.2) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<0.1.0->datasets==1.10.2) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<0.1.0->datasets==1.10.2) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets==1.10.2) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.10.2) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.10.2) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.10.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets==1.10.2) (2022.9.24)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.10.2) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets==1.10.2) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==1.10.2) (1.15.0)\n",
            "Installing collected packages: xxhash, multiprocess, datasets\n",
            "Successfully installed datasets-1.10.2 multiprocess-0.70.14 xxhash-3.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "import csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def write_tsv_dataframe(filepath, dataframe):\n",
        "    \"\"\"\n",
        "        Stores `DataFrame` as tsv file\n",
        "        Parameters\n",
        "        ----------\n",
        "        filepath : str\n",
        "            Path to tsv file\n",
        "        dataframe : pd.DataFrame\n",
        "            DataFrame to store\n",
        "        Raises\n",
        "        ------\n",
        "        IOError\n",
        "            if the file can't be opened\n",
        "    \"\"\"\n",
        "    try:\n",
        "        dataframe.to_csv(filepath, encoding='utf-8', sep='\\t', index=False, header=True, quoting=csv.QUOTE_NONE)\n",
        "    except IOError:\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "KecnfChlbXWZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "def combine_columns(df_arguments, df_labels):\n",
        "    \"\"\"Combines the two `DataFrames` on column `Argument ID`\"\"\"\n",
        "    return pd.merge(df_arguments, df_labels, on='Argument ID')\n",
        "\n",
        "\n",
        "def split_arguments(df_arguments):\n",
        "    \"\"\"Splits `DataFrame` by column `Usage` into `train`-, `validation`-, and `test`-arguments\"\"\"\n",
        "    train_arguments = df_arguments.loc[df_arguments['Usage'] == 'train'].drop(['Usage'], axis=1).reset_index(drop=True)\n",
        "    valid_arguments = df_arguments.loc[df_arguments['Usage'] == 'validation'].drop(['Usage'], axis=1).reset_index(drop=True)\n",
        "    test_arguments = df_arguments.loc[df_arguments['Usage'] == 'test'].drop(['Usage'], axis=1).reset_index(drop=True)\n",
        "    \n",
        "    return train_arguments, valid_arguments, test_arguments\n",
        "\n",
        "\n",
        "def create_dataframe_head(argument_ids, model_name):\n",
        "    \"\"\"\n",
        "        Creates `DataFrame` usable to append predictions to it\n",
        "        Parameters\n",
        "        ----------\n",
        "        argument_ids : list[str]\n",
        "            First column of the resulting DataFrame\n",
        "        model_name : str\n",
        "            Second column of DataFrame will contain the given model name\n",
        "        Returns\n",
        "        -------\n",
        "        pd.DataFrame\n",
        "            prepared DataFrame\n",
        "    \"\"\"\n",
        "    df_model_head = pd.DataFrame(argument_ids, columns=['Argument ID'])\n",
        "    df_model_head['Method'] = [model_name] * len(argument_ids)\n",
        "\n",
        "    return df_model_head"
      ],
      "metadata": {
        "id": "ELZVdd-HblYY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import traceback\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "\n",
        "class MissingColumnError(AttributeError):\n",
        "    \"\"\"Error indicating that an imported DataFrame lacks necessary columns\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "def load_json_file(filepath):\n",
        "    \"\"\"Load content of json-file from `filepath`\"\"\"\n",
        "    with open(filepath, 'r') as  json_file:\n",
        "        return json.load(json_file)\n",
        "\n",
        "\n",
        "def load_values_from_json(filepath):\n",
        "    \"\"\"Load values per level from json-file from `filepath`\"\"\"\n",
        "    json_values = load_json_file(filepath)\n",
        "    values = { \"1\":set(), \"2\":set(), \"3\":set(), \"4a\":set(), \"4b\":set() }\n",
        "    for value in json_values[\"values\"]:\n",
        "        values[\"1\"].add(value[\"name\"])\n",
        "        values[\"2\"].add(value[\"level2\"])\n",
        "        for valueLevel3 in value[\"level3\"]:\n",
        "            values[\"3\"].add(valueLevel3)\n",
        "        for valueLevel4a in value[\"level4a\"]:\n",
        "            values[\"4a\"].add(valueLevel4a)\n",
        "        for valueLevel4b in value[\"level4b\"]:\n",
        "            values[\"4b\"].add(valueLevel4b)\n",
        "    values[\"1\"] = sorted(values[\"1\"])\n",
        "    values[\"2\"] = sorted(values[\"2\"])\n",
        "    values[\"3\"] = sorted(values[\"3\"])\n",
        "    values[\"4a\"] = sorted(values[\"4a\"])\n",
        "    values[\"4b\"] = sorted(values[\"4b\"])\n",
        "    return values\n",
        "\n",
        "\n",
        "def load_arguments_from_tsv(filepath, default_usage='test'):\n",
        "    \"\"\"\n",
        "        Reads arguments from tsv file\n",
        "        Parameters\n",
        "        ----------\n",
        "        filepath : str\n",
        "            The path to the tsv file\n",
        "        default_usage : str, optional\n",
        "            The default value if the column \"Usage\" is missing\n",
        "        Returns\n",
        "        -------\n",
        "        pd.DataFrame\n",
        "            the DataFrame with all arguments\n",
        "        Raises\n",
        "        ------\n",
        "        MissingColumnError\n",
        "            if the required columns \"Argument ID\" or \"Premise\" are missing in the read data\n",
        "        IOError\n",
        "            if the file can't be read\n",
        "        \"\"\"\n",
        "    try:\n",
        "        dataframe = pd.read_csv(filepath, encoding='utf-8', sep='\\t', header=0)\n",
        "        if not {'Argument ID', 'Premise'}.issubset(set(dataframe.columns.values)):\n",
        "            raise MissingColumnError('The argument \"%s\" file does not contain the minimum required columns [Argument ID, Premise].' % filepath)\n",
        "        if 'Usage' not in dataframe.columns.values:\n",
        "            dataframe['Usage'] = [default_usage] * len(dataframe)\n",
        "        return dataframe\n",
        "    except IOError:\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_labels_from_tsv(filepath, label_order):\n",
        "    \"\"\"\n",
        "        Reads label annotations from tsv file\n",
        "        Parameters\n",
        "        ----------\n",
        "        filepath : str\n",
        "            The path to the tsv file\n",
        "        label_order : list[str]\n",
        "            The listing and order of the labels to use from the read data\n",
        "        Returns\n",
        "        -------\n",
        "        pd.DataFrame\n",
        "            the DataFrame with the annotations\n",
        "        Raises\n",
        "        ------\n",
        "        MissingColumnError\n",
        "            if the required columns \"Argument ID\" or names from `label_order` are missing in the read data\n",
        "        IOError\n",
        "            if the file can't be read\n",
        "        \"\"\"\n",
        "    try:\n",
        "        dataframe = pd.read_csv(filepath, encoding='utf-8', sep='\\t', header=0)\n",
        "        dataframe = dataframe[['Argument ID'] + label_order]\n",
        "        return dataframe\n",
        "    except IOError:\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "    except KeyError:\n",
        "        raise MissingColumnError('The file \"%s\" does not contain the required columns for its level.' % filepath)"
      ],
      "metadata": {
        "id": "qAo55SKrbq1d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ufzKD0NvDxGa",
        "outputId": "ced9e44e-b815-4802-8afe-a6e0d65b6dda"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 3.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from datasets import (Dataset, DatasetDict, load_dataset)\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification,\n",
        "                          PreTrainedModel, BertModel, BertForSequenceClassification,\n",
        "                          TrainingArguments, Trainer)\n",
        "from sklearn.metrics import f1_score\n",
        "# from transformers import MBartTokenizer\n",
        "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertTokenizerFast\n",
        "from transformers import RobertaTokenizer\n",
        "from transformers import LongformerTokenizer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def accuracy_thresh(y_pred, y_true, thresh=0.5, sigmoid=True):\n",
        "    \"\"\"Compute accuracy of predictions\"\"\"\n",
        "    y_pred = torch.from_numpy(y_pred)\n",
        "    y_true = torch.from_numpy(y_true)\n",
        "    if sigmoid:\n",
        "        y_pred = y_pred.sigmoid()\n",
        "\n",
        "    return ((y_pred > thresh) == y_true.bool()).float().mean().item()\n",
        "\n",
        "\n",
        "def f1_score_per_label(y_pred, y_true, value_classes, thresh=0.5, sigmoid=True):\n",
        "    \"\"\"Compute label-wise and averaged F1-scores\"\"\"\n",
        "    y_pred = torch.from_numpy(y_pred)\n",
        "    y_true = torch.from_numpy(y_true)\n",
        "    if sigmoid:\n",
        "        y_pred = y_pred.sigmoid()\n",
        "\n",
        "    y_true = y_true.bool().numpy()\n",
        "    y_pred = (y_pred > thresh).numpy()\n",
        "\n",
        "    f1_scores = {}\n",
        "    for i, v in enumerate(value_classes):\n",
        "        f1_scores[v] = round(f1_score(y_true[:, i], y_pred[:, i], zero_division=0), 2)\n",
        "\n",
        "    f1_scores['avg-f1-score'] = round(np.mean(list(f1_scores.values())), 2)\n",
        "\n",
        "    return f1_scores\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred, value_classes):\n",
        "    \"\"\"Custom metric calculation function for MultiLabelTrainer\"\"\"\n",
        "    predictions, labels = eval_pred\n",
        "    f1scores = f1_score_per_label(predictions, labels, value_classes)\n",
        "    return {'accuracy_thresh': accuracy_thresh(predictions, labels), 'f1-score': f1scores,\n",
        "            'marco-avg-f1score': f1scores['avg-f1-score']}\n",
        "\n",
        "\n",
        "class MultiLabelTrainer(Trainer):\n",
        "    \"\"\"\n",
        "        A transformers `Trainer` with custom loss computation\n",
        "        Methods\n",
        "        -------\n",
        "        compute_loss(model, inputs, return_outputs=False):\n",
        "            Overrides loss computation from Trainer class\n",
        "        \"\"\"\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        \"\"\"Custom loss computation\"\"\"\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        loss_fct = torch.nn.BCEWithLogitsLoss()\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels),\n",
        "                        labels.float().view(-1, self.model.config.num_labels))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "def tokenize_and_encode(examples):\n",
        "    \"\"\"Tokenizes each arguments \"Premise\" \"\"\"\n",
        "    return tokenizer(examples['Premise'], truncation=True)\n",
        "\n",
        "\n",
        "def convert_to_dataset(train_dataframe, test_dataframe, labels):\n",
        "    \"\"\"\n",
        "        Converts pandas DataFrames into a DatasetDict\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_dataframe : pd.DataFrame\n",
        "            Arguments to be listed as \"train\"\n",
        "        test_dataframe : pd.DataFrame\n",
        "            Arguments to be listed as \"test\"\n",
        "        labels : list[str]\n",
        "            The labels in both DataFrames\n",
        "        Returns\n",
        "        -------\n",
        "        tuple(DatasetDict, list[str])\n",
        "            a `DatasetDict` with attributes \"train\" and \"test\" for the listed arguments,\n",
        "            a `list` with the contained labels\n",
        "        \"\"\"\n",
        "    column_intersect = [x for x in (['Premise'] + labels) if x in train_dataframe.columns.values]\n",
        "\n",
        "    train_dataset = Dataset.from_dict((train_dataframe[column_intersect]).to_dict('list'))\n",
        "    test_dataset = Dataset.from_dict((test_dataframe[column_intersect]).to_dict('list'))\n",
        "\n",
        "    ds = DatasetDict()\n",
        "    ds['train'] = train_dataset\n",
        "    ds['test'] = test_dataset\n",
        "\n",
        "    ds = ds.map(lambda x: {\"labels\": [int(x[c]) for c in ds['train'].column_names if\n",
        "                                      c not in ['Argument ID', 'Conclusion', 'Stance', 'Premise', 'Part']]})\n",
        "\n",
        "    cols = ds['train'].column_names\n",
        "    cols.remove('labels')\n",
        "\n",
        "    ds_enc = ds.map(tokenize_and_encode, batched=True, remove_columns=cols)\n",
        "\n",
        "    cols.remove('Premise')\n",
        "\n",
        "    return ds_enc, cols\n",
        "\n",
        "\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bert-uncased\")\n",
        "# tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\") 6\n",
        "# tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\") 10\n",
        "\n",
        "# tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n",
        "\n",
        "\n",
        "def load_model_from_data_dir(model_dir, num_labels):\n",
        "    \"\"\"Loads Bert model from specified directory and converts to CUDA model if available\"\"\"\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=num_labels)\n",
        "    if torch.cuda.is_available():\n",
        "        return model.to('cuda')\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict_bert_model(dataframe, model_dir, labels):\n",
        "    \"\"\"\n",
        "        Classifies each argument using the Bert model stored in `model_dir`\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataframe: pd.Dataframe\n",
        "            The arguments to be classified\n",
        "        model_dir: str\n",
        "            The directory of the pre-trained Bert model to use\n",
        "        labels: list[str]\n",
        "            The labels to predict\n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            numpy nd-array with the predictions given by the model\n",
        "        \"\"\"\n",
        "    ds, no_labels = convert_to_dataset(dataframe, dataframe, labels)\n",
        "    num_labels = len(labels)\n",
        "    ds = ds.remove_columns(['labels'])\n",
        "\n",
        "    batch_size = 8\n",
        "    args = TrainingArguments(\n",
        "        output_dir=model_dir,\n",
        "        do_train=False,\n",
        "        do_eval=False,\n",
        "        do_predict=True,\n",
        "        per_device_eval_batch_size=batch_size\n",
        "    )\n",
        "\n",
        "    model = load_model_from_data_dir(model_dir, num_labels=num_labels)\n",
        "\n",
        "    multi_trainer = MultiLabelTrainer(\n",
        "        model,\n",
        "        args,\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    prediction = 1 * (multi_trainer.predict(ds['train']).predictions > 0.5)\n",
        "\n",
        "    return prediction\n",
        "\n",
        "\n",
        "def train_bert_model(train_dataframe, model_dir, labels, test_dataframe=None, num_train_epochs=20):\n",
        "    \"\"\"\n",
        "        Trains Bert model with the arguments in `train_dataframe`\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_dataframe: pd.DataFrame\n",
        "            The arguments to be trained on\n",
        "        model_dir: str\n",
        "            The directory for storing the trained model\n",
        "        labels : list[str]\n",
        "            The labels in the training data\n",
        "        test_dataframe: pd.DataFrame, optional\n",
        "            The validation arguments (default is None)\n",
        "        num_train_epochs: int, optional\n",
        "            The number of training epochs (default is 20)\n",
        "        Returns\n",
        "        -------\n",
        "        Metrics\n",
        "            result of validation if `test_dataframe` is not None\n",
        "        NoneType\n",
        "            otherwise\n",
        "        \"\"\"\n",
        "    if test_dataframe is None:\n",
        "        test_dataframe = train_dataframe\n",
        "    ds, labels = convert_to_dataset(train_dataframe, test_dataframe, labels)\n",
        "\n",
        "    batch_size = 8\n",
        "\n",
        "    args = TrainingArguments(\n",
        "        output_dir=model_dir,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        learning_rate=2e-5,\n",
        "        per_device_train_batch_size=batch_size,\n",
        "        per_device_eval_batch_size=batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        weight_decay=0.01,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model='marco-avg-f1score'\n",
        "    )\n",
        "\n",
        "    model = load_model_from_data_dir(\"allenai/longformer-base-4096\", num_labels=len(labels))\n",
        "\n",
        "    multi_trainer = MultiLabelTrainer(\n",
        "        model,\n",
        "        args,\n",
        "        train_dataset=ds[\"train\"],\n",
        "        eval_dataset=ds[\"test\"],\n",
        "        compute_metrics=lambda x: compute_metrics(x, labels),\n",
        "        tokenizer=tokenizer\n",
        "    )\n",
        "\n",
        "    multi_trainer.train()\n",
        "\n",
        "    model.save_pretrained(model_dir)\n",
        "\n",
        "    if test_dataframe is not None:\n",
        "        return multi_trainer.evaluate()"
      ],
      "metadata": {
        "id": "wZn8OrnUbxFb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def predict_one_baseline(dataframe, labels):\n",
        "    \"\"\"\n",
        "        Classifies each argument in the dataframe as corresponding to all labels.\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataframe : pd.DataFrame\n",
        "            The arguments to be classified\n",
        "        labels : list[str]\n",
        "            The listing of all labels\n",
        "        Returns\n",
        "        -------\n",
        "        DataFrame\n",
        "            the predictions given by the model\n",
        "        \"\"\"\n",
        "    return pd.DataFrame(np.full((len(dataframe), len(labels)), 1, dtype=int), columns=labels)"
      ],
      "metadata": {
        "id": "ne6d7HWQcBt4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import json\n",
        "\n",
        "# constant label values\n",
        "vocab_label = 'vocabulary'\n",
        "idf_label = 'idf'\n",
        "intercept_label = 'intercept'\n",
        "coef_label = 'coef'\n",
        "\n",
        "\n",
        "class MyLinearSVC(LinearSVC):\n",
        "    \"\"\"\n",
        "        A class to load and store a pretrained linear svm for predictions\n",
        "        ...\n",
        "        Attributes\n",
        "        ----------\n",
        "        intercept : int\n",
        "            The intercept constant\n",
        "        coef : ndarray\n",
        "            The coefficients for all observed features\n",
        "        Methods\n",
        "        -------\n",
        "        predict(X):\n",
        "            Overrides `predict(X)` from LinearSVC\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, intercept, coef):\n",
        "        \"\"\"\n",
        "            Constructs all necessary attributes for the MyLinearSVC object\n",
        "            Parameters\n",
        "            ----------\n",
        "            intercept : int\n",
        "                The intercept constant\n",
        "            coef : list[int]\n",
        "                The coefficients for all observed features\n",
        "        \"\"\"\n",
        "        LinearSVC.__init__(self, C=24, class_weight='balanced', max_iter=10000)\n",
        "        self.intercept = intercept\n",
        "        self.coef = np.asarray(coef)\n",
        "        self.size = len(coef)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "            Predict class labels for samples in X\n",
        "            Parameters\n",
        "            ----------\n",
        "            X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
        "                The data matrix for which we want to get the predictions\n",
        "            Returns\n",
        "            -------\n",
        "            ndarray of shape (n_samples,)\n",
        "                Vector containing the class labels for each sample\n",
        "        \"\"\"\n",
        "        input = np.transpose(X.todense())\n",
        "        matrix = np.squeeze(np.asarray(self.__my_predict(input)))\n",
        "        return np.vectorize(lambda x: 1 if x >= 0.5 else 0)(matrix)\n",
        "\n",
        "    def __my_predict(self, input):\n",
        "        \"\"\"\n",
        "            Predict class label probability for samples in input\n",
        "            Parameters\n",
        "            ----------\n",
        "            input : ndarray of shape (n_features, n_samples)\n",
        "                The data matrix for which we want to get the predictions\n",
        "            Returns\n",
        "            -------\n",
        "            ndarray of shape (n_samples,)\n",
        "                Vector containing the class labels for each sample\n",
        "        \"\"\"\n",
        "        result = input[0] * self.coef[0]\n",
        "        for i in range(1, self.size):\n",
        "            result += input[i] * self.coef[i]\n",
        "        result += self.intercept\n",
        "        return result\n",
        "\n",
        "\n",
        "def predict_svm(dataframe, labels, vectorizer_file, model_file):\n",
        "    \"\"\"\n",
        "        Classifies each argument in the dataframe using the trained Support Vector Machines (SVMs) in the `model_file`\n",
        "        Parameters\n",
        "        ----------\n",
        "        dataframe : pd.DataFrame\n",
        "            The arguments to be classified\n",
        "        labels : list[str]\n",
        "            The listing of all labels\n",
        "        vectorizer_file : str\n",
        "            The file containing the fitted data from the TfidfVectorizer\n",
        "        model_file : str\n",
        "            The file containing the serialized SVM models\n",
        "        Returns\n",
        "        -------\n",
        "        DataFrame\n",
        "            the predictions given by the model\n",
        "        \"\"\"\n",
        "    input_vector = dataframe['Premise']\n",
        "    df_model_predictions = {}\n",
        "\n",
        "    # load vectorizer\n",
        "    with open(vectorizer_file, \"r\") as f:\n",
        "        vectorizer_json = json.load(f)\n",
        "\n",
        "    vocabulary = vectorizer_json[vocab_label]\n",
        "    idf = np.asarray(vectorizer_json[idf_label])\n",
        "\n",
        "    vectorizer = TfidfVectorizer(vocabulary=vocabulary)\n",
        "    # vectorizer = CountVectorizer(vocabulary=vocabulary)\n",
        "    # vectors = vectorizer.fit_transform(corpus)\n",
        "    vectorizer.idf_ = idf\n",
        "\n",
        "    with open(model_file, \"r\") as f:\n",
        "        model_json = json.load(f)\n",
        "\n",
        "    for label_name in labels:\n",
        "        model_dict = model_json[label_name]\n",
        "        svm = Pipeline([\n",
        "            ('tfidf', vectorizer),\n",
        "            ('clf', MyLinearSVC(intercept=model_dict[intercept_label], coef=model_dict[coef_label])),\n",
        "        ])\n",
        "        df_model_predictions[label_name] = svm.predict(input_vector)\n",
        "\n",
        "    return pd.DataFrame(df_model_predictions, columns=labels)\n",
        "\n",
        "\n",
        "def train_svm(train_dataframe, labels, vectorizer_file, model_file, test_dataframe=None):\n",
        "    \"\"\"\n",
        "        Trains Support Vector Machines (SVMs) on the arguments in the train_dataframe and saves them in `model_file`\n",
        "        Parameters\n",
        "        ----------\n",
        "        train_dataframe : pd.DataFrame\n",
        "            The arguments to be trained on\n",
        "        labels : list[str]\n",
        "            The listing of all labels\n",
        "        vectorizer_file : str\n",
        "            The file for storing the fitted data from the TfidfVectorizer\n",
        "        model_file : str\n",
        "            The file for storing the serialized SVM models\n",
        "        test_dataframe : pd.DataFrame, optional\n",
        "            The validation arguments (default is None)\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            f1-scores of validation if `test_dataframe` is not None\n",
        "        NoneType\n",
        "            otherwise\n",
        "        \"\"\"\n",
        "    train_input_vector = train_dataframe['Premise']\n",
        "    if test_dataframe is not None:\n",
        "        valid_input_vector = test_dataframe['Premise']\n",
        "        f1_scores = {}\n",
        "\n",
        "    # vectorizer = CountVectorizer(stop_words='english')\n",
        "    vectorizer = TfidfVectorizer(stop_words='english')\n",
        "    vectorizer.fit(train_input_vector)\n",
        "\n",
        "    vectorizer_json = {vocab_label: vectorizer.vocabulary_, idf_label: vectorizer.idf_.tolist()}\n",
        "\n",
        "    with open(vectorizer_file, \"w\") as f:\n",
        "        json.dump(vectorizer_json, f)\n",
        "\n",
        "    # dictionary for storing model data\n",
        "    model_json = {}\n",
        "\n",
        "    for label_name in labels:\n",
        "        svm = Pipeline([\n",
        "            ('tfidf', vectorizer),\n",
        "            ('clf', OneVsRestClassifier(LinearSVC(C=24, class_weight='balanced', max_iter=10000), n_jobs=1)),\n",
        "        ])\n",
        "        svm.fit(train_input_vector, train_dataframe[label_name])\n",
        "\n",
        "        coef = np.squeeze(np.asarray(svm.steps[1][1].estimators_[0].coef_)).tolist()\n",
        "        intercept = svm.steps[1][1].estimators_[0].intercept_[0]\n",
        "        model_json[label_name] = {intercept_label: intercept, coef_label: coef}\n",
        "\n",
        "        if test_dataframe is not None:\n",
        "            valid_pred = svm.predict(valid_input_vector)\n",
        "            f1_scores[label_name] = round(f1_score(test_dataframe[label_name], valid_pred, zero_division=0), 2)\n",
        "\n",
        "    with open(model_file, \"w\") as f:\n",
        "        json.dump(model_json, f)\n",
        "\n",
        "    if test_dataframe is not None:\n",
        "        f1_scores['avg-f1-score'] = round(np.mean(list(f1_scores.values())), 2)\n",
        "        return f1_scores"
      ],
      "metadata": {
        "id": "Y3jI6VElcRfy"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import getopt\n",
        "import os\n",
        "\n",
        "# from components.setup import (load_values_from_json, load_arguments_from_tsv, load_labels_from_tsv,\n",
        "#                                                 combine_columns, split_arguments)\n",
        "# from components.models import (train_bert_model, train_svm)\n",
        "\n",
        "help_string = '\\nUsage:  training.py [OPTIONS]' \\\n",
        "              '\\n' \\\n",
        "              '\\nTrain the BERT model (and optional SVM) on the arguments' \\\n",
        "              '\\n' \\\n",
        "              '\\nOptions:' \\\n",
        "              '\\n  -c, --classifier string  Select classifier: \"b\" for Bert, \"s\" for SVM, \"bs\" for both (default' \\\n",
        "              '\\n                           \"b\")' \\\n",
        "              '\\n  -d, --data-dir string    Directory with the argument files (default \"/data/\")' \\\n",
        "              '\\n  -h, --help               Display help text' \\\n",
        "              '\\n  -l, --levels string      Comma-separated list of taxonomy levels to train models for (default' \\\n",
        "              '\\n                           \"1,2,3,4a,4b\")' \\\n",
        "              '\\n  -m, --model-dir string   Directory for saving the trained models (default \"/models/\")' \\\n",
        "              '\\n  -v, --validate           Request evaluation after training'\n",
        "\n",
        "\n",
        "def main():\n",
        "    # default values\n",
        "    curr_dir = os.getcwd()\n",
        "    run_bert = False\n",
        "    run_svm = True\n",
        "    data_dir = '/content/data_dir'\n",
        "    levels = [\"1\", \"2\", \"3\", \"4a\", \"4b\"]\n",
        "    model_dir = '/content/models/'\n",
        "    validate = False\n",
        "\n",
        "    # try:\n",
        "    #     opts, args = getopt.gnu_getopt(argv, \"c:d:hl:m:v\", [\"classifier=\", \"data-dir=\", \"help\", \"levels=\", \"model-dir=\", \"validate\"])\n",
        "    # except getopt.GetoptError:\n",
        "    #     print(help_string)\n",
        "    #     sys.exit(2)\n",
        "    # for opt, arg in opts:\n",
        "    #     if opt in ('-h', '--help'):\n",
        "    #         print(help_string)\n",
        "    #         sys.exit()\n",
        "    #     elif opt in ('-c', '--classifier'):\n",
        "    #         run_bert = 'b' in arg.lower()\n",
        "    #         run_svm = 's' in arg.lower()\n",
        "    #         if not run_bert and not run_svm:\n",
        "    #             print('No classifiers selected')\n",
        "    #             sys.exit(2)\n",
        "    #     elif opt in ('-d', '--data-dir'):\n",
        "    #         data_dir = arg\n",
        "    #     elif opt in ('-l', '--levels'):\n",
        "    #         levels = arg.split(\",\")\n",
        "    #     elif opt in ('-m', '--model-dir'):\n",
        "    #         model_dir = arg\n",
        "    #     elif opt in ('-v', '--validate'):\n",
        "    #         validate = True\n",
        "\n",
        "    svm_dir = os.path.join(model_dir, 'svm')\n",
        "\n",
        "    # Check data directory\n",
        "    if not os.path.isdir(data_dir):\n",
        "        print('The specified data directory \"%s\" does not exist' % data_dir)\n",
        "        sys.exit(2)\n",
        "\n",
        "    # Check model directory\n",
        "    if os.path.isfile(model_dir):\n",
        "        print('The specified <model-dir> \"%s\" points to an existing file' % model_dir)\n",
        "        sys.exit(2)\n",
        "    if os.path.isdir(model_dir) and len(os.listdir(model_dir)) > 0:\n",
        "        print('The specified <model-dir> \"%s\" already exists and contains files' % model_dir)\n",
        "        decision = input('Do You still want to proceed? [y/n]\\n').lower()\n",
        "        if decision != 'y':\n",
        "            sys.exit(-1)\n",
        "    if not os.path.exists(model_dir):\n",
        "        os.makedirs(model_dir)\n",
        "\n",
        "    if run_svm and not os.path.isdir(svm_dir):\n",
        "        if os.path.exists(svm_dir):\n",
        "            print('Unable to create svm directory at \"%s\"' % svm_dir)\n",
        "        else:\n",
        "            os.mkdir(svm_dir)\n",
        "\n",
        "    argument_filepath = os.path.join(data_dir, 'arguments.tsv')\n",
        "    value_json_filepath = os.path.join(data_dir, 'values.json')\n",
        "\n",
        "    if not os.path.isfile(argument_filepath):\n",
        "        print('The required file \"arguments.tsv\" is not present in the data directory')\n",
        "        sys.exit(2)\n",
        "    if not os.path.isfile(value_json_filepath):\n",
        "        print('The required file \"values.json\" is not present in the data directory')\n",
        "        sys.exit(2)\n",
        "\n",
        "    # load arguments\n",
        "    df_arguments = load_arguments_from_tsv(argument_filepath, default_usage='train')\n",
        "    if len(df_arguments) < 1:\n",
        "        print('There are no arguments in file \"%s\"' % argument_filepath)\n",
        "        sys.exit(2)\n",
        "\n",
        "    values = load_values_from_json(value_json_filepath)\n",
        "    num_levels = len(levels)\n",
        "\n",
        "    # check levels\n",
        "    for i in range(num_levels):\n",
        "        if levels[i] not in values:\n",
        "            print('Missing attribute \"{}\" in value.json'.format(levels[i]))\n",
        "            sys.exit(2)\n",
        "\n",
        "    # format dataset\n",
        "    df_train_all = []\n",
        "    df_valid_all = []\n",
        "    for i in range(num_levels):\n",
        "        label_filepath = os.path.join(data_dir, 'labels-level{}.tsv'.format(levels[i]))\n",
        "        if not os.path.isfile(label_filepath):\n",
        "            print('The required file \"labels-level{}.tsv\" is not present in the data directory'.format(levels[i]))\n",
        "            sys.exit(2)\n",
        "        # read labels from .tsv file\n",
        "        df_labels = load_labels_from_tsv(label_filepath, values[levels[i]])\n",
        "        # join arguments and labels\n",
        "        df_full_level = combine_columns(df_arguments, df_labels)\n",
        "        # split dataframe by usage\n",
        "        train_arguments, valid_arguments, _ = split_arguments(df_full_level)\n",
        "        df_train_all.append(train_arguments)\n",
        "        df_valid_all.append(valid_arguments)\n",
        "\n",
        "    if len(df_train_all[0]) < 1:\n",
        "        print('There are no arguments listed for training.')\n",
        "        sys.exit()\n",
        "\n",
        "    if validate and len(df_valid_all[0]) < 1:\n",
        "        print('There are no arguments listed for validation. Proceeding without validation.')\n",
        "        validate = False\n",
        "\n",
        "    # train bert model\n",
        "    if run_bert:\n",
        "        for i in range(num_levels):\n",
        "            print(\"===> Bert: Training Level %s...\" % levels[i])\n",
        "            if validate:\n",
        "                bert_model_evaluation = train_bert_model(df_train_all[i],\n",
        "                                                         os.path.join(model_dir, 'bert_train_level{}'.format(levels[i])),\n",
        "                                                         values[levels[i]], test_dataframe=df_valid_all[i])\n",
        "                print(\"F1-Scores for Level %s:\" % levels[i])\n",
        "                print(bert_model_evaluation['eval_f1-score'])\n",
        "            else:\n",
        "                train_bert_model(df_train_all[i], os.path.join(model_dir, 'bert_train_level{}'.format(levels[i])),\n",
        "                                 values[levels[i]])\n",
        "\n",
        "    if run_svm:\n",
        "        for i in range(num_levels):\n",
        "            print(\"===> SVM: Training Level %s...\" % levels[i])\n",
        "            if validate:\n",
        "                svm_f1_scores = train_svm(df_train_all[i], values[levels[i]],\n",
        "                                          os.path.join(model_dir, 'svm/svm_train_level{}_vectorizer.json'.format(levels[i])),\n",
        "                                          os.path.join(model_dir, 'svm/svm_train_level{}_models.json'.format(levels[i])),\n",
        "                                          test_dataframe=df_valid_all[i])\n",
        "                print(\"F1-Scores for Level %s:\" % levels[i])\n",
        "                print(svm_f1_scores)\n",
        "            else:\n",
        "                train_svm(df_train_all[i], values[levels[i]],\n",
        "                          os.path.join(model_dir, 'svm/svm_train_level{}_vectorizer.json'.format(levels[i])),\n",
        "                          os.path.join(model_dir, 'svm/svm_train_level{}_models.json'.format(levels[i])))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-EN8BEgcWb7",
        "outputId": "3efbcf22-7bba-4dc2-b473-75646ae3abf7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The specified <model-dir> \"/content/models/\" already exists and contains files\n",
            "Do You still want to proceed? [y/n]\n",
            "y\n",
            "===> SVM: Training Level 1...\n",
            "===> SVM: Training Level 2...\n",
            "===> SVM: Training Level 3...\n",
            "===> SVM: Training Level 4a...\n",
            "===> SVM: Training Level 4b...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import getopt\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# from components.setup import (load_values_from_json, load_arguments_from_tsv, split_arguments,\n",
        "#                               write_tsv_dataframe, create_dataframe_head)\n",
        "# from components.models import (predict_bert_model, predict_one_baseline, predict_svm)\n",
        "\n",
        "help_string = '\\nUsage:  predict.py [OPTIONS]' \\\n",
        "              '\\n' \\\n",
        "              '\\nRequest prediction of the BERT model (and optional SVM / 1-Baseline) for all test arguments' \\\n",
        "              '\\n' \\\n",
        "              '\\nOptions:' \\\n",
        "              '\\n  -c, --classifier string  Select classifier: \"b\" for Bert, \"s\" for SVM, \"o\" for 1-Baseline,' \\\n",
        "              '\\n                           or combination like \"so\" (default \"b\")' \\\n",
        "              '\\n  -d, --data-dir string    Directory with the argument files (default \"/data/\")' \\\n",
        "              '\\n  -h, --help               Display help text' \\\n",
        "              '\\n  -l, --levels string      Comma-separated list of taxonomy levels to train models for (default' \\\n",
        "              '\\n                           \"1,2,3,4a,4b\")' \\\n",
        "              '\\n  -m, --model-dir string   Directory for saving the trained models (default \"/models/\")' \\\n",
        "              '\\n  -o, --output-dir string  Directory to write the \"predictions.tsv\" into (default \"/output/\")'\n",
        "\n",
        "\n",
        "def main():\n",
        "    # default values\n",
        "    curr_dir = os.getcwd()\n",
        "    run_bert = False\n",
        "    run_svm = True\n",
        "    run_one_baseline = False\n",
        "    data_dir = '/content/data_dir'\n",
        "    levels = [\"1\", \"2\", \"3\", \"4a\", \"4b\"]\n",
        "    model_dir = '/content/models/'\n",
        "    output_dir = '/content/output/'\n",
        "\n",
        "    # try:\n",
        "    #     opts, args = getopt.gnu_getopt(argv, \"c:d:hl:m:o:\",\n",
        "    #                                    [\"classifier=\", \"data-dir=\", \"help\", \"levels=\", \"model-dir=\", \"output-dir=\"])\n",
        "    # except getopt.GetoptError:\n",
        "    #     print(help_string)\n",
        "    #     sys.exit(2)\n",
        "    # for opt, arg in opts:\n",
        "    #     if opt in ('-h', '--help'):\n",
        "    #         print(help_string)\n",
        "    #         sys.exit()\n",
        "    #     elif opt in ('-c', '--classifier'):\n",
        "    #         run_bert = 'b' in arg.lower()\n",
        "    #         run_svm = 's' in arg.lower()\n",
        "    #         run_one_baseline = 'o' in arg.lower()\n",
        "    #         if not run_bert and not run_svm and not run_one_baseline:\n",
        "    #             print('No classifiers selected')\n",
        "    #             sys.exit(2)\n",
        "    #     elif opt in ('-d', '--data-dir'):\n",
        "    #         data_dir = arg\n",
        "    #     elif opt in ('-l', '--levels'):\n",
        "    #         levels = arg.split(\",\")\n",
        "    #     elif opt in ('-m', '--model-dir'):\n",
        "    #         model_dir = arg\n",
        "    #     elif opt in ('-o', '--output-dir'):\n",
        "    #         output_dir = arg\n",
        "\n",
        "    # Check data directory\n",
        "    if not os.path.isdir(data_dir):\n",
        "        print('The specified data directory \"%s\" does not exist' % data_dir)\n",
        "        sys.exit(2)\n",
        "\n",
        "    argument_filepath = os.path.join(data_dir, 'arguments.tsv')\n",
        "    values_filepath = os.path.join(data_dir, 'values.json')\n",
        "\n",
        "    if not os.path.isfile(argument_filepath):\n",
        "        print('The required file \"arguments.tsv\" is not present in the data directory')\n",
        "        sys.exit(2)\n",
        "    if not os.path.isfile(values_filepath):\n",
        "        print('The required file \"values.json\" is not present in the data directory')\n",
        "        sys.exit(2)\n",
        "\n",
        "    # load arguments\n",
        "    df_arguments = load_arguments_from_tsv(argument_filepath)\n",
        "    if len(df_arguments) < 1:\n",
        "        print('There are no arguments in file \"%s\"' % argument_filepath)\n",
        "        sys.exit(2)\n",
        "\n",
        "    values = load_values_from_json(values_filepath)\n",
        "    num_levels = len(levels)\n",
        "\n",
        "    # check levels\n",
        "    for i in range(num_levels):\n",
        "        if levels[i] not in values:\n",
        "            print('Missing attribute \"{}\" in value.json'.format(levels[i]))\n",
        "            sys.exit(2)\n",
        "\n",
        "    # check model directory\n",
        "    if not os.path.isdir(model_dir):\n",
        "        print('The specified <model-dir> \"%s\" does not exist' % model_dir)\n",
        "        sys.exit(2)\n",
        "\n",
        "    for i in range(num_levels):\n",
        "        if run_bert and not os.path.exists(os.path.join(model_dir, 'bert_train_level{}'.format(levels[i]))):\n",
        "            print('Missing saved Bert model for level \"{}\"'.format(levels[i]))\n",
        "            sys.exit(2)\n",
        "        if run_svm and (\n",
        "                not os.path.exists(os.path.join(model_dir, 'svm/svm_train_level{}_vectorizer.json'.format(levels[i])))\n",
        "                and not os.path.exists(os.path.join(model_dir, 'svm/svm_train_level{}_models.json'.format(levels[i])))):\n",
        "            print('Missing saved SVM models for level \"{}\"'.format(levels[i]))\n",
        "            sys.exit(2)\n",
        "\n",
        "    # format dataset\n",
        "    _, _, df_test = split_arguments(df_arguments)\n",
        "\n",
        "    if len(df_test) < 1:\n",
        "        print('There are no arguments listed for prediction.')\n",
        "        sys.exit()\n",
        "\n",
        "    # predict with Bert model\n",
        "    if run_bert:\n",
        "        df_bert = create_dataframe_head(df_test['Argument ID'], model_name='Bert')\n",
        "        for i in range(num_levels):\n",
        "            print(\"===> Bert: Predicting Level %s...\" % levels[i])\n",
        "            result = predict_bert_model(df_test, os.path.join(model_dir, 'bert_train_level{}'.format(levels[i])),\n",
        "                                        values[levels[i]])\n",
        "            df_bert = pd.concat([df_bert, pd.DataFrame(result, columns=values[levels[i]])], axis=1)\n",
        "        df_prediction = df_bert\n",
        "\n",
        "    # predict with SVM\n",
        "    if run_svm:\n",
        "        df_svm = create_dataframe_head(df_test['Argument ID'], model_name='SVM')\n",
        "        for i in range(num_levels):\n",
        "            print(\"===> SVM: Predicting Level %s...\" % levels[i])\n",
        "            result = predict_svm(df_test, values[levels[i]],\n",
        "                                 os.path.join(model_dir, 'svm/svm_train_level{}_vectorizer.json'.format(levels[i])),\n",
        "                                 os.path.join(model_dir, 'svm/svm_train_level{}_models.json'.format(levels[i])))\n",
        "            df_svm = pd.concat([df_svm, result], axis=1)\n",
        "\n",
        "        if not run_bert:\n",
        "            df_prediction = df_svm\n",
        "        else:\n",
        "            df_prediction = pd.concat([df_prediction, df_svm])\n",
        "\n",
        "    # predict with 1-Baseline\n",
        "    if run_one_baseline:\n",
        "        df_one_baseline = create_dataframe_head(df_test['Argument ID'], model_name='1-Baseline')\n",
        "        for i in range(num_levels):\n",
        "            print(\"===> 1-Baseline: Predicting Level %s...\" % levels[i])\n",
        "            result = predict_one_baseline(df_test, values[levels[i]])\n",
        "            df_one_baseline = pd.concat([df_one_baseline, result], axis=1)\n",
        "\n",
        "        if not run_bert and not run_svm:\n",
        "            df_prediction = df_one_baseline\n",
        "        else:\n",
        "            df_prediction = pd.concat([df_prediction, df_one_baseline])\n",
        "\n",
        "    # write predictions\n",
        "    print(\"===> Writing predictions...\")\n",
        "    write_tsv_dataframe(os.path.join(output_dir, 'predictions_c_24.tsv'), df_prediction)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Gk4XRR65C19",
        "outputId": "9a25ccc4-96a2-46c4-aeed-3aac39256ed5"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===> SVM: Predicting Level 1...\n",
            "===> SVM: Predicting Level 2...\n",
            "===> SVM: Predicting Level 3...\n",
            "===> SVM: Predicting Level 4a...\n",
            "===> SVM: Predicting Level 4b...\n",
            "===> Writing predictions...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ImwydfUHPViV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}